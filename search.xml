<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>无向图求解生成树个数</title>
      <link href="/blog/2023/04/06/%E6%97%A0%E5%90%91%E5%9B%BE%E6%B1%82%E8%A7%A3%E7%94%9F%E6%88%90%E6%A0%91%E4%B8%AA%E6%95%B0/"/>
      <url>/blog/2023/04/06/%E6%97%A0%E5%90%91%E5%9B%BE%E6%B1%82%E8%A7%A3%E7%94%9F%E6%88%90%E6%A0%91%E4%B8%AA%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>本文章着重讲如何<strong>根据无向图计算出它的生成树个数</strong>，但不会讲实现原理，也就是<strong>不会论证</strong>。</p><h2 id="能看懂本文章的基本要求"><a href="#能看懂本文章的基本要求" class="headerlink" title="能看懂本文章的基本要求"></a>能看懂本文章的基本要求</h2><p>0、知道什么是无向图</p><p>1、知道什么是生成树</p><p>2、知道如何根据矩阵算出下三角矩阵</p><p>3、知道余子式</p><h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><p>假设现在有一个题目，要求计算出该无向图的生成树个数。</p><p><img src="image-20230403222547666.png" alt="image-20230403222547666"></p><p>对于求解无向图生成树个数问题，我们可以分成两类。</p><h3 id="第一类"><a href="#第一类" class="headerlink" title="第一类"></a>第一类</h3><p>也是最简单的一类，即给了你一个完全图，让你求它的生成树个数，我们知道一个**完全图（即每个点和其他点都有边相连）的生成树个数为n^(n-2)**，n为顶点数，所以对于这类问题，直接通过公式算出。</p><h3 id="第二类"><a href="#第二类" class="headerlink" title="第二类"></a>第二类</h3><p>这一类则比较复杂，如果简单点的可能通过肉眼就能判断出来，但是对于上图很难通过肉眼直接判断出生成树的个数到底有多少个，对于这类问题，我们需要使用拉<strong>普拉斯矩阵</strong>(Laplacian matrix)也叫做<strong>基尔霍夫矩阵</strong>（并不可怕，请不要被吓到）。</p><p>拉普拉斯矩阵的定义为<strong>L &#x3D; D - A</strong>，<strong>L的任意余子式即为无向图的生成树个数</strong>。</p><h2 id="拉普拉斯矩阵：L-x3D-D-A"><a href="#拉普拉斯矩阵：L-x3D-D-A" class="headerlink" title="拉普拉斯矩阵：L &#x3D; D - A"></a>拉普拉斯矩阵：L &#x3D; D - A</h2><h3 id="A"><a href="#A" class="headerlink" title="A"></a>A</h3><p>其中A为邻接矩阵，我们以下面截图为例子，第一行为0，1，0，0，0，1，代表的是顶点1和顶点2、顶点6相连的意思。</p><p><img src="image-20230403223212607.png" alt="image-20230403223212607"></p><h3 id="D"><a href="#D" class="headerlink" title="D"></a>D</h3><p>D即把每一列的数字相加，然后把列总和放到对角线上，然后让其他位置变为全0，它也称之为度矩阵，对角线上代表每个顶点的度数。</p><p><img src="image-20230403223125302.png" alt="image-20230403223125302"></p><h3 id="根据公式-L-x3D-D-A"><a href="#根据公式-L-x3D-D-A" class="headerlink" title="根据公式:L &#x3D; D - A"></a>根据公式:L &#x3D; D - A</h3><p><img src="image-20230403222822070.png" alt="image-20230403222822070"></p><h3 id="余子式（我们选择去掉第2行第2列）"><a href="#余子式（我们选择去掉第2行第2列）" class="headerlink" title="余子式（我们选择去掉第2行第2列）"></a>余子式（我们选择去掉第2行第2列）</h3><p><img src="image-20230403223038453.png" alt="image-20230403223038453"></p><h3 id="然后计算出下三角"><a href="#然后计算出下三角" class="headerlink" title="然后计算出下三角"></a>然后计算出下三角</h3><p><img src="image-20230403222147739.png" alt="image-20230403222147739"></p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>我们把对角线上的数字相乘后，就能得出该无向图生成树个数为15。</p><h2 id="一些或许有帮助的网址"><a href="#一些或许有帮助的网址" class="headerlink" title="一些或许有帮助的网址"></a>一些或许有帮助的网址</h2><pre><code class="bash">自定义生成图：https://csacademy.com/app/graph_editor/矩阵计算：https://matrixcalc.org/zh/生成树概念+矩阵代码：https://blog.csdn.net/qq_40438165/article/details/88526546拉普拉斯矩阵：https://www.cnblogs.com/pupil-xj/p/11714407.html</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>商业和开源的faas伸缩性比较</title>
      <link href="/blog/2023/04/03/%E5%95%86%E4%B8%9A%E5%92%8C%E5%BC%80%E6%BA%90%E7%9A%84faas%E4%BC%B8%E7%BC%A9%E6%80%A7%E6%AF%94%E8%BE%83/"/>
      <url>/blog/2023/04/03/%E5%95%86%E4%B8%9A%E5%92%8C%E5%BC%80%E6%BA%90%E7%9A%84faas%E4%BC%B8%E7%BC%A9%E6%80%A7%E6%AF%94%E8%BE%83/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem-motivation"><a href="#Problem-motivation" class="headerlink" title="Problem motivation"></a><strong>Problem motivation</strong></h2><p>最近faas越来越火，将来或许会成为主流，因为faas让开发者更关注于代码开发，不用负责服务器的管理，并且易于拓展和伸缩。为了与时俱进，跟上时代洪流，我想要调查并研究已经存在的faas实现方案，并对它的伸缩性等性能做测试、分析和总结。</p><h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a><strong>Related work</strong></h2><p>调查发现Faas有商用的平台的和开源的方案，因此我觉得在商用和开源各选择一个比较有代表性的faas作品来做测试、分析。</p><p>市面上目前faas商用做的比较好的有Aws lambda，google cloud funtions，azure functions，开源做的比较好的是openfaas，因此商用我选用azure functions，开源选择openfaas的。</p><p>我们准备调查研究商业用的azure faas和开源的openfaas在性能，伸缩性等有什么区别。</p><h4 id="商用faas（azure-functions）的相关工作"><a href="#商用faas（azure-functions）的相关工作" class="headerlink" title="商用faas（azure functions）的相关工作"></a>商用faas（azure functions）的相关工作</h4><p>登录azure，使用了学校的订阅，并使用了学校分配的资源组，创建了funtion，选择的语言是python，为了方便开发和测试，我可以使用两种方式连接和部署azure function。第一种：本地下载visual studio code，通过拓展库的azure function连接azure，然后编写python函数代码；第二种是直接通过azure提供的平台去编写python代码。</p><h4 id="Openfaas的相关工作"><a href="#Openfaas的相关工作" class="headerlink" title="Openfaas的相关工作"></a>Openfaas的相关工作</h4><p>在azure创建一个k8s服务，研究如何部署openfaas，并对外暴露端口，如何远程拉取docker hub的镜像，如何使用metrics server对流量监控，实现自动伸缩的能力，对docker，k8s等命令和原理做了解，研究如何使用faas-cli。</p><h2 id="Experimental-design"><a href="#Experimental-design" class="headerlink" title="Experimental design"></a><strong>Experimental design</strong></h2><p>本次实验主要是对商用的faas（选用azure function）和开源的openfaas做性能测试（主要是对伸缩性做测试），实验会测试两个方面，一个是测试cpu的伸缩性；第二个是测试内存的伸缩性。</p><h4 id="对于测试cpu"><a href="#对于测试cpu" class="headerlink" title="对于测试cpu"></a>对于测试cpu</h4><p>我会选择做nxn的矩阵乘法，并且n会固定传1000，矩阵随机生成，每个请求耗时都在500-600毫秒之间，我会通过jmeter开启多线程在1200秒内一共会有24000个线程一起请求对应的function地址；因为是在大流量的情况下，可能会出现响应失败的情况，这个时候我会着重观察是否在一段时间后可以恢复响应；如果成功返回，那么我会在返回的主体中，返回当前主机的mac地址，以便让我知道faas是否做了自动伸缩，以保证它的稳定性，可用性。</p><h4 id="对于测试内存"><a href="#对于测试内存" class="headerlink" title="对于测试内存"></a>对于测试内存</h4><p>我会选择写文件，通过开启jmeter多线程并发访问对应的function，function做的主要工作是，使用io流对文件进行写入，字符的数量是20000000，和测试cpu时的类似，字符串长度和字符串本身的字符都是随机生成的，但是为了避免大批量的写文件导致文件过度堆积或者避免手动删除的负担，我会选择写入&#x2F;tmp里，因为linux对这块区域使用了定时清理，所以让我更放心的去测试，同时我也会观察是否正常响应，如果正常响应我也还是会打印出它的mac地址，已确认是否扩容，扩容的量是多少；如果没有正常返回，我还是会继续多线程访问，直到它扩容到足够能成功响应的时候，并记录它的时间。</p><p>共同的部分：</p><p>为了保证函数的健壮性，需要对可能异常的代码块做好try except防止程序异常导致崩溃。对于程序的出入口，出现异常跳转的代码块都要做好日志埋点，方便跟踪整个链路，以便还原发生的事件。</p><p>为了测试伸缩性，我会使用apache的Jmeter，通过设置线程组，http请求来并发的发送请求，通过增加监听器：观察树，聚合报告，响应图来检验和分析整个过程，最终得出结论。</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h2><h4 id="Azure-function"><a href="#Azure-function" class="headerlink" title="Azure function"></a>Azure function</h4><p>我们通过vs code安装azure function的拓展插件，通过跳转网页登录后，选择function，并在本地构造一个工作区后，可以开始编写function了，按照上述说的，我主要选择两种方式测试它的性能（主要是伸缩性），第一是cpu密集型的任务，第二是对内存和磁盘io的。cpu密集型任务我编写的两个1000x1000矩阵相乘，计算任务做完后，我会返回当前机器的mac地址，以便让我确认是否有扩容的操作，并且整体时间我控制在500-600毫秒返回，当大并发的情况下，它不得不扩容才能成功的返回响应。对于内存和磁盘io做伸缩性测试，主要是做写文件的操作，因为磁盘非常大，挺难做扩容的，所以主要还是对内存做压力测试，看看是否有扩容操作，python有一个库专门是针对每个系统的临时存储区做一些io操作的，这个库名是tempfile，通过tempfile我们就不用手动定位临时文件区的位置，直接通过这个库就能实现，我固定往里面写入20000000个字符串，相信20000000字符串就算存入内存中也是一笔开销，并且我还通过并发去请求，几千几万流量打进来，内存再大肯定也顶不住，得做扩容才行。通过vs code很容易就把代码上传到azure function即可调用测试，非常方便。</p><h4 id="Openfaas"><a href="#Openfaas" class="headerlink" title="Openfaas"></a>Openfaas</h4><p>要部署openfaas就比azure function要困难和麻烦的多了，首先我们需要借助k8s的能力，在azure建立了一个k8s服务，并且选择自动扩容（这是很关键的一步），我选择的最小节点池大小为1，最大为105，通过azure本地的shell登录进了k8s机器里，通过helm下载了openfaas服务，并且通过kubectl命令部署了openfaas，并且对外提供访问的能力，接下来我们下载faas-cli，这个是openfaas的客户端，我们可以把代码通过faas-cli去和openfaas做交互并部署函数。通过faas-cli新创建一个函数，faas-cli会帮我们生成好handler.py, yaml文件，requirements.txt文件等，然后通过faas-cli build构建好整个环境，faas-cli push把function打包成镜像上传到docker hub上保存，然后可以通过faas-cli deploy把上传到docker hub上的镜像拉下来部署，并且会返回部署成功的ip地址和端口号。比较方便的一点是k8s自带了metrics server，所以流量监控这块已经做好准备，我们只需要设置触发扩容的阈值就可以了，通过kubectl hpa可以设置对外暴露服务的扩容阈值，并且最大和最小节点数都会打印出来。代码和azure function所部署的类似，所以就不做再次说明了，不同的可能是返回体，因为azure在python上有自己的库，也有自己的返回类型，而在openfaas我可能只是简单的返回str类型。</p><p>并且不管是openfaas还是azure function我都做好了日志输出，在函数被调用的时候，出现异常的时候，正常返回的时候，都做了日志埋点，方便跟踪整个链路。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a><strong>Results</strong></h2><p>[贴图]（感兴趣的可以通过邮箱向我要源码和过程截图）</p><h2 id="Quality-of-evaluation"><a href="#Quality-of-evaluation" class="headerlink" title="Quality of evaluation"></a><strong>Quality of evaluation</strong></h2><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>Azure function和openfaas都是用于构建无服务的工具，本实验其实有非常多的局限性，所以只能做参考。因为尽管openfaas和azure function都有着伸缩性，可以根据当前请求量等做自动缩放，但是azure functions毕竟是一个完全托管的服务，并且有着强大的azure平台支撑，是按照流量计算消费，所以伸缩性在理论上可以理解成无限大。但是我们的openfaas需要借助k8s的自动扩缩容能力，通过metrics server收集cpu使用率和内存使用率也可以做到自动缩放（根据需要增加或减少资源），如果只是瞬间的高并发，可能就不会实现扩容，并且扩容需要一定的时间，在这段时间里，如果生产者远远慢于消费者，那么还是会出现服务不响应的问题。所以对于openfaas的测试我会在一段时间内持续的访问，以便看到它扩容的过程。</p><h4 id="cpu"><a href="#cpu" class="headerlink" title="cpu"></a>cpu</h4><p>对于openfaas的cpu伸缩性测试来说，我们可以发现，在没有通过网络访问之前，cpu的指标维持在比较低的一个水平，在我们开始启动多线程，不断的进行加压访问后（比如一秒启动10个线程），马上就开始出现崩溃的情况，频繁的出现socket连接问题，k8s过了一段时间才逐渐反应过来，我们可以从图片中看到，k8s在控制节点池的增加，pod也在开始不断的扩容。过了一段时间后（大约4分钟），k8s已经扩容到了足够接纳我访问的地步，渐渐的能响应我的访问，我们可以看到cpu一开始维持在比较高的水平，在扩容节点后，节点池中的每一个节点都承受了一部分访问压力，做到了负载均衡，同时我们可以看到响应时间一开始非常高，后面就在某个区间来回波动了。并且在我断开jmeter的压力测试，节点池和pods根据当前cpu的情况都在不断的缩容，最终缩到一个节点池和一个pod。</p><p>而azure function令我比较惊讶的是，对于同一个function它一开始承受的压力远远比我对openfaas施加的压力大2倍，但一开始也没有报错，通过图片可以看出来，所有的http请求都得到正面的回应，并且我们看到返回的mac地址，都是不同的主机做的返回，也就是说azure function的扩容能力非常强大，同时我也尝试了过了半个小时后使用一个线程不断的同步访问，发现都是返回的同一个mac地址，所以azure function一开始就只部署在了一台主机上，并且在一开始就做到了应对如此大的流量，并且它的扩容时间非常的短暂，非常的令我惊讶，同时我们可以发现响应时间图一直在某个区间波动，比较的平稳。</p><h4 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h4><p>对于openfaas的内存伸缩性测试，我们可以看到，在访问之前，我们先部署到了k8s上，并且在jmeter创建好脚本，准备做并发访问，当前的内存使用率维持在35%左右，随后我们开始做并发压力访问，很快的出现了测试cpu伸缩性时候出现的问题，刚开始或许能成功的做一些返回，但是随后随机流量的加大，很快就出现了请求失败的问题，一方面是因为访问一次所需要的时间的确比较久，大概1-2s才能返回，第二个是因为它的扩容需要时间，我们能发现过了一端时间后，大概5分钟，他就能承受住我们的流量，访问后开始逐步的能返回结果了，我们可以看到内存值在不断的波动，并且达到设置的阈值后，不断的扩容节点池和pods，使得每个pod都维持在阈值以下，并且时间响应图能看出来，响应时间在某个时间达到了高峰，后面就下降到一个区域，维持成了一条平稳的直线。在关闭了脚本，关闭了多线程后，我们发现节点池在变少，pods也在不断缩容。</p><p>Azure function的内存伸缩性测试和cpu伸缩性测试一样，让人非常的赞叹，持续的正常响应，它的毫秒级内存波动也是挺大的，时间响应在一开始达到了巅峰，但是在短短20秒内做到了快速的下降，并且维持一条直线，实在是令我惊讶不已。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>总的来说，azure function有着强大的平台做支撑，伸缩能力强，扩容时间短，文档全面，并且有实时的技术支持，各方面都做到比较完善，但是毕竟是商用，所以收费是需要的。</p><p>Openfaas也有着自己的社区和全面的文档，但是在技术实现上，可能需要借助k8s或者使用openfaas pro实现自动伸缩的能力，并且扩容缩容的时间还需要配置好，以免出现流量过大导致扩容不及时，更好的做法可能是在一开始就部署一部分的节点，以应对一开始的大流量。同时openfaas是开源的，所以如果想要寻求帮助，解决可能没有azure专人解答高效。但毕竟是开源的，在大家的努力下，将来或许会成为一款性能顶级的faas实现方案。</p><h2 id="Code-x2F-scripts"><a href="#Code-x2F-scripts" class="headerlink" title="Code&#x2F;scripts"></a><strong>Code&#x2F;scripts</strong></h2><p>[贴代码]（感兴趣的可以通过邮箱向我要源码和过程截图）</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>部署+使用集群的算力跑CPU密集型任务</title>
      <link href="/blog/2023/03/20/%E9%83%A8%E7%BD%B2-%E4%BD%BF%E7%94%A8%E9%9B%86%E7%BE%A4%E7%9A%84%E7%AE%97%E5%8A%9B%E8%B7%91CPU%E5%AF%86%E9%9B%86%E5%9E%8B%E4%BB%BB%E5%8A%A1/"/>
      <url>/blog/2023/03/20/%E9%83%A8%E7%BD%B2-%E4%BD%BF%E7%94%A8%E9%9B%86%E7%BE%A4%E7%9A%84%E7%AE%97%E5%8A%9B%E8%B7%91CPU%E5%AF%86%E9%9B%86%E5%9E%8B%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<p>我先在开头做一个总结，表达我最终要做的事情和最终环境是如何的，然后我会一步步说明我是如何搭建。</p><h1 id="要做的事情"><a href="#要做的事情" class="headerlink" title="要做的事情"></a>要做的事情</h1><p>尝试如何使用<strong>多台机器的算力</strong>共同跑一个CPU密集型或者GPU密集型的任务。这里以CPU密集型为例子。</p><p>在多台机器搭建MPI环境，构建<strong>MPI集群</strong>共同跑<strong>1亿个数据的快排任务</strong>，并且对机器的各种指标（如CPU，内存，磁盘，网络等）做<strong>可视化监控</strong>。</p><h1 id="最终环境"><a href="#最终环境" class="headerlink" title="最终环境"></a>最终环境</h1><p>我这里选择的云平台是Microsoft Azure，使用<strong>两台机器做实验</strong>，两台操作系统信息一致为：Linux (<strong>ubuntu 18.04</strong>) Standard B2ms (2 vcpu，8 GiB 内存)，两台虚拟机实现了<strong>ssh免密互通</strong>，通过<strong>nfs</strong>在其中一台虚拟机创建共享空间，让另一个虚拟机可以<strong>远程挂载</strong>访问，两台虚拟机<strong>通过rpc进行通信</strong>，两台虚拟机都配置好了可以运行<strong>MPI的环境</strong>，配置好了<strong>prometheus+node_exporter+grafana</strong>，每个服务所需要的<strong>端口</strong>都已经通过azure安全组打开。</p><h1 id="如何配置？"><a href="#如何配置？" class="headerlink" title="如何配置？"></a>如何配置？</h1><h2 id="1-虚拟机初始化"><a href="#1-虚拟机初始化" class="headerlink" title="1.虚拟机初始化"></a>1.虚拟机初始化</h2><p>我选用了Microsoft Azure的产品，订阅了UoL-Teaching-SOC-MCC后，找到了给我分配的资源组：* uol_feps_soc_comp5850m_xxxxxx。（<strong>学校给我分配的，大家可能要自己去购买使用</strong>）我在上面构建了两台虚拟机，一台叫做<strong>jhvm</strong>（51.11.167.xx），另一台叫做<strong>jhvm2</strong>（20.254.126.xx），同时在两台虚拟机上我都创建了同样的<strong>用户名mppi</strong>，为了保证<strong>登录的安全性</strong>，我选择<strong>使用密钥进行登录</strong>，即：</p><pre><code>ssh -i jhvm_key.pem mppi@51.11.167.xx</code></pre><p>即公钥会保存在服务器上，私钥则在本地，在创建虚拟机的时候，Azure会问你是否使用非对称密钥进行登录，选择是后，即可下载.pem文件。</p><h2 id="2-如何实现两个虚拟机的免密登录"><a href="#2-如何实现两个虚拟机的免密登录" class="headerlink" title="2.如何实现两个虚拟机的免密登录"></a>2.如何实现两个虚拟机的免密登录</h2><p>首先为了<strong>不再使用ip而是使用昵称</strong>，我在&#x2F;etc&#x2F;hosts上增加了：</p><pre><code class="bash">51.11.167.xx node120.254.126.xx node2</code></pre><p>&#x2F;etc&#x2F;ssh&#x2F;sshd_config中设置</p><pre><code class="bash">PubkeyAuthentication yes</code></pre><p>确保开启了密钥登录。对node1和node2节点的<strong>家目录设置权限为700</strong>，否则可能会存在<strong>免密登录失败</strong>的问题，在node1上，我通过</p><pre><code class="bash">ssh-keygen -t rsa</code></pre><p>生成公钥私钥到.ssh目录，把**.ssh目录权限设置为600<strong>，主要</strong>不能有写权限**，不然可能会免密失败，然后通过</p><pre><code class="bash">ssh-copy-id -i .ssh/id_rsa.pub mppi@node1ssh-copy-id -i .ssh/id_rsa.pub mppi@node2</code></pre><p>命令后，输入了node2用户mppi的密码，把公钥放到了node2的.ssh&#x2F;authorized_keys文件中，同时再使用<strong>scp命令把私钥也复制给node2节点</strong>(node1也要，因为他会放进authorized_keys文件中，让node2也能免密登录node1)。</p><pre><code class="bash">scp .ssh/id_rsa node2:/home/mppi/.ssh</code></pre><p>至此，node1和node2都同时有了公钥和私钥，我们输入ssh node2命令，即可免密登录进node2，node2使用ssh node1也能免密登录进node1。</p><h2 id="3-mpi集群环境搭建"><a href="#3-mpi集群环境搭建" class="headerlink" title="3.mpi集群环境搭建"></a>3.mpi集群环境搭建</h2><p>切换到root权限，我们执行</p><pre><code class="bash">apt-get install mpich</code></pre><p>命令即可下载mpi环境。把qsort.c放到&#x2F;home&#x2F;mppi&#x2F;mpi_share里。<strong>这里不会对mpi原理过多讲解，主要还是如何使用工具搭建好要的环境。</strong>感兴趣的话大家可以google或者baidu等自行搜索。这里的代码主要作用是获取Input.txt文件的内容：先获取文件的第一个数字n，代表有n个数，然后获取n个数，即我随机生成的n个数，然后会把排序好的数据放到output.txt中，input.txt和output.txt名字可以通过参数指定。mpi可以通过-n指定cpu个数。</p><p>对mpiexec(mpirun)感兴趣的同学，可以通过man mpiexec(mpirun)去查看说明文档。</p><pre><code class="c++">/*   qsort.c - Parallel sorting algorithm based on quicksort   Original code by Hans-Wolfgang Loidl   Heriot-Watt University, Edinburgh   Adapted by Karim Djemame   Execution time includes input, processing and output   February 2023   compile: mpicc -Wall -o qsort qsort.c   run:     mpirun -np num_procs qsort in_file out_file   num_procs: number of processors   in_file: input file to sort   out_file: result file   Example: on a single machine:   mpirun -np 2 qsort input.txt output.txt*/#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;mpi.h&gt;#include &lt;time.h&gt;double startTime;/* swap entries in array v at positions i and j; used by quicksort */static inline /* this improves performance; Exercise: by how much? */void swap(int *v, int i, int j) &#123;    int t = v[i];    v[i] = v[j];    v[j] = t;&#125;/* (quick) sort slice of array v; slice starts at s and is of length n */void quicksort(int *v, int s, int n) &#123;    int x, p, i;    // base case?    if (n &lt;= 1)        return;    // pick pivot and swap with first element    x = v[s + n / 2];    swap(v, s, s + n / 2);    // partition slice starting at s+1    p = s;    for (i = s + 1; i &lt; s + n; i++)        if (v[i] &lt; x) &#123;            p++;            swap(v, i, p);        &#125;    // swap pivot into place    swap(v, s, p);    // recurse into partition    quicksort(v, s, p - s);    quicksort(v, p + 1, s + n - p - 1);&#125;/* merge two sorted arrays v1, v2 of lengths n1, n2, respectively */int *merge(int *v1, int n1, int *v2, int n2) &#123;    int *result = (int *)malloc((n1 + n2) * sizeof(int));    int i = 0;    int j = 0;    int k;    for (k = 0; k &lt; n1 + n2; k++) &#123;        if (i &gt;= n1) &#123;            result[k] = v2[j];            j++;        &#125; else if (j &gt;= n2) &#123;            result[k] = v1[i];            i++;        &#125; else if (v1[i] &lt; v2[j]) &#123; // indices in bounds as i &lt; n1 &amp;&amp; j &lt; n2            result[k] = v1[i];            i++;        &#125; else &#123; // v2[j] &lt;= v1[i]            result[k] = v2[j];            j++;        &#125;    &#125;    return result;&#125;int main(int argc, char **argv) &#123;    int n;    int *data = NULL;    int c, s;    int *chunk;    int o;    int *other;    int step;    int p, id;    MPI_Status status;    double elapsed_time;    FILE *file = NULL;    int i;    if (argc != 3) &#123;        fprintf(stderr, &quot;Usage: mpirun -np &lt;num_procs&gt; %s &lt;in_file&gt; &lt;out_file&gt;\n&quot;, argv[0]);        exit(1);    &#125;    MPI_Init(&amp;argc, &amp;argv);    MPI_Comm_size(MPI_COMM_WORLD, &amp;p);    MPI_Comm_rank(MPI_COMM_WORLD, &amp;id);    char processorname[100];    int namelen;    MPI_Get_processor_name(processorname, &amp;namelen);    printf(&quot;processor %i of %i running on machine %s\n&quot;, id, p, processorname);    MPI_Barrier(MPI_COMM_WORLD);    elapsed_time = - MPI_Wtime();    if (id == 0) &#123;        // read size of data        file = fopen(argv[1], &quot;r&quot;);        fscanf(file, &quot;%d&quot;, &amp;n);        // compute chunk size        c = (n % p != 0) ? n / p + 1 : n / p;        // read data from file        data = (int *)malloc(p * c * sizeof(int));        for (i = 0; i &lt; n; i++)            fscanf(file, &quot;%d&quot;, &amp;(data[i]));        fclose(file);        // pad data with 0 -- doesn&#39;t matter        for (i = n; i &lt; p * c; i++)            data[i] = 0;    &#125;    // start the timer//  MPI_Barrier(MPI_COMM_WORLD);//  elapsed_time = - MPI_Wtime();    // broadcast size    MPI_Bcast(&amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD);    // compute chunk size    c = (n % p != 0) ? n / p + 1 : n / p;    // scatter data    chunk = (int *)malloc(c * sizeof(int));    MPI_Scatter(data, c, MPI_INT, chunk, c, MPI_INT, 0, MPI_COMM_WORLD);    free(data);    data = NULL;    // compute size of own chunk and sort it    s = (n &gt;= c * (id + 1)) ? c : n - c * id;    quicksort(chunk, 0, s);    // up to log_2 p merge steps    for (step = 1; step &lt; p; step = 2 * step) &#123;        if (id % (2 * step) != 0) &#123;            // id is no multiple of 2*step: send chunk to id-step and exit loop            MPI_Send(chunk, s, MPI_INT, id - step, 0, MPI_COMM_WORLD);            break;        &#125;        // id is multiple of 2*step: merge in chunk from id+step (if it exists)        if (id + step &lt; p) &#123;            // compute size of chunk to be received            o = (n &gt;= c * (id + 2 * step)) ? c * step : n - c * (id + step);            // receive other chunk            other = (int *)malloc(o * sizeof(int));            MPI_Recv(other, o, MPI_INT, id + step, 0, MPI_COMM_WORLD, &amp;status);            // merge and free memory            data = merge(chunk, s, other, o);            free(chunk);            free(other);            chunk = data;            s = s + o;        &#125;    &#125;    // stop the timer// elapsed_time += MPI_Wtime();    // write sorted data to out file and print out timer    if (id == 0) &#123;        file = fopen(argv[2], &quot;w&quot;);        fprintf(file, &quot;%d\n&quot;, s);   // assert (s == n)        for (i = 0; i &lt; s; i++)            fprintf(file, &quot;%d\n&quot;, chunk[i]);        fclose(file);// stop the timer        elapsed_time += MPI_Wtime();        printf(&quot;Quicksort %d ints on %d procs: %f secs\n&quot;, n, p, elapsed_time);    &#125;    MPI_Finalize();    return 0;&#125;</code></pre><p>同时在<strong>同一个目录下添加mpi_config文件</strong>，输入(即node1和node2都开启两个cpu去跑的意思)</p><pre><code class="bash">node1:2node2:2</code></pre><p>然后为了测试我们mpi集群是否搭建成功，我们按照qsort.c文件的入参要求，设置了input.txt和output.txt，其中input.txt里我放了1 million个数字做测试，第一个代表要排序数字的个数n，后面跟着n个数。然后为了测试是否是成功运行在两台机器上，把qsort.c通过命令编译成qsort：</p><pre><code class="bash">mpicc qsort.c -o qsort</code></pre><p>通过mpiexec -n 4 -f mpi_config .&#x2F;qsort 1_million.txt 1_output.txt命令执行后，通过输出发现jhvm和jhvm2都分别有2个进程在做运算，说明我的环境搭建成功。</p><p><img src="image-20230320111024714.png" alt="image-20230320111024714"></p><p>失败的同学可能是mpi需要的端口没开。</p><h2 id="4-nfs和rpcbind搭建"><a href="#4-nfs和rpcbind搭建" class="headerlink" title="4.nfs和rpcbind搭建"></a>4.nfs和rpcbind搭建</h2><p>NFS(Network File System)主要功能是通过网络来做文件存储，使用<strong>NFS可以实现多台服务器之间数据共享</strong>，NFS之间<strong>通过rpc进行通信</strong>。这里同样不会对原理过多讲解，主要还是如何使用工具搭建好要的环境。感兴趣的话大家可以google或者baidu等自行搜索。</p><p>通过</p><pre><code class="bash">apt-get install nfs-kernel-serverapt-get install rpcbind# 失败的同学可以先执行apt-get update, 刷新源索引列表</code></pre><p>命令下载好nfs共享目录的工具和rpc通信方式（启动的时候需要先启动rpc，因为nfs需要先找到rpc去绑定），我选择使用node1节点作为主节点，修改&#x2F;etc&#x2F;exports文件，添加配置：</p><pre><code class="bash">/home/mppi/mpi_share node1(rw,sync,no_root_squash,no_subtree_check)/home/mppi/mpi_share node2(rw,sync,no_root_squash,no_subtree_check)# 可参考 https://blog.csdn.net/weixin_45361475/article/details/117754118# 可参考 http://events.jianshu.io/p/3035c7636d23</code></pre><p>里面的地址就是要共享目录的位置，然后我们分别在node1和node2的这个位置去创建文件夹mpi_share，然后<strong>启动node1节点的rpc服务</strong>，<strong>再启动</strong>nfs-server服务，node2也需要启动rpc并通过</p><pre><code class="bash">mount -t nfs node1:/home/mppi/mpi_share</code></pre><p>命令挂载到node1同位置目录上。然后我尝试在node1节点创建了一个文件，在node2同位置路径下也出现了相同文件，说明我搭建成功了，我们以后有任何要计算的任务，可以只把文件复制到node1节点上即可，不需要手动的去复制到node2，或者集群扩充后的node3等等，非常的方便。</p><p><strong>不使用nfs+rpc其实mpi也能跑</strong>，但是需要自己手动复制文件到node2节点，用mpi跑的时候直接**-hosts**即可：</p><pre><code class="bash">mpirun -n 4 -hosts node1:2,node2:2 ./qsort input.txt output.txt</code></pre><h2 id="5-node-exporter"><a href="#5-node-exporter" class="headerlink" title="5.node_exporter"></a>5.node_exporter</h2><p>通过教程了解并且搭建好node_exporter, prometheus, Grafana，这三者的关系是：<strong>prometheus是可以通过node_exporter获取到多个机器的各种指标信息，Grafana是对prometheus的可视化。</strong></p><p>搭建教程：<a href="https://medium.com/devops-dudes/install-prometheus-on-ubuntu-18-04-a51602c6256b">https://medium.com/devops-dudes/install-prometheus-on-ubuntu-18-04-a51602c6256b</a></p><p>通过教程在整合过程中，我发现教程给的node_exporter版本号太低了，所以我后面自己重新下载了0.18.1版本的node_exporter（因为在grafana很多现成的dashboard都需要node_exporter版本0.18或以上），随后跟着教程把node_exporter二进制文件放到&#x2F;usr&#x2F;local&#x2F;bin里管理，同时为node_exporter构建一个不可登录的用户去管理，然后在创建&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;node_exporter.service，配置好unit，service和install后，重新加载守护进程，运行node_exporter即可。</p><h2 id="6-prometheus"><a href="#6-prometheus" class="headerlink" title="6.prometheus"></a>6.prometheus</h2><p>也是通过给的教程，安装了2.1.0版本的prometheus，同样也是把二进制文件放到&#x2F;usr&#x2F;local&#x2F;bin里，为prometheus创建一些数据目录：&#x2F;etc&#x2F;prometheus &#x2F;var&#x2F;lib&#x2F;prometheus，把一些配置文件，像consoles或者console_libraries等放到&#x2F;etc&#x2F;prometheus里。</p><p>添加prometheus的配置文件：&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml，设置抓取速率等，配置job_name和指标可用的端口号，为prometheus也创建一个不可登录的账号进行管理，并修改权限，以免被修改。随后也创建prometheus.service配置好unit，service和install，重新加载systemd，然后运行prometheus即可。通过ip:9090成功显示网页，环境成功搭建。</p><h2 id="7-Grafana"><a href="#7-Grafana" class="headerlink" title="7.Grafana"></a>7.Grafana</h2><p>这里和node_exporter一样，我没有使用教程给的版本号，而是使用的9.2.3版本，因为5.0.4实在太老了，很多dashboard不能用。解压后root下通过systemctl daemon-reload &amp;&amp; systemctl 启用 grafana-server &amp;&amp; systemctl start grafana-server.service即可。通过url：ip:3000有dashboard界面，我成功搭建了grafana。在grafana指定了data sources后，输入了对应的ip:9090，并且我在 <a href="https://grafana.com/grafana/dashboards/?dataSource=prometheus">https://grafana.com/grafana/dashboards/?dataSource=prometheus</a> 找到了一个非常合适的、同时也是非常流行的dashboard：node exporter full，可以可视化主机上很多的性能指标，比如cpu、内存、磁盘、网络等等。配置好后即可显示vm上的各项指标。</p><h1 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h1><p>我会分为两个部分，第一个部分则是最为简单的<strong>单个虚拟机情况</strong>，第二个则是<strong>双虚拟机集群</strong>情况。</p><p>我会分别尝试单机开启2，4，8个进程去跑100万、500万、1000万和2000万的快排数据，也就是说会有3x4&#x3D;12个输出。比如说2个进程的时候跑100万、500万、1000万和2000万的随机生成数据。</p><p>生成数据的原则：当要生成100万数据的时候，我随机生成的范围为0-200万，跑500万、1000万和2000万数据的时候，随机的范围则和他们的数据量大小相等。</p><p>单机部分我会根据本虚拟机cpu个数，和开启的进程个数，快排算法跑完时间做对比才考，并且会通过grafana可视化cpu、磁盘、内存的情况，做出分析和总结。</p><p>在第二个集群跑快排的部分，我同样会有12个输出，3种不同的进程数量尝试4种不同数量大小的快排。同样会去<strong>根据两边cpu的总个数，一共开启的进程数，通过grafana去查看cpu、内存、磁盘去思考+总结</strong>。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="单台"><a href="#单台" class="headerlink" title="单台"></a>单台</h2><p>在一台具有2cpu的虚拟机上，我们尝试在不同数量的进程下运行100万、500万、1000万和2000万个随机数的数据大小。我们发现，只有当<strong>CPU数量&#x3D;进程数量时，快速排序的效率最高</strong>，并且已经验证了计算密集型工作与CPU之间的关系是平等的，效率更高。我们还注意到，只有在运行数千万数据时，内存才会略有波动。如果不使用mpi等高效的流程协作工具，也不使用快速排序等高效算法，那么内存使用和CPU使用将更加明显。一个是因为CPU需要很长时间，另一个是由于数千万的数据本身已经是数百兆字节。</p><p><img src="image-20230320112625490.png" alt="image-20230320112625490"></p><p>node1 grafana:</p><p><img src="image-20230320112631610.png" alt="image-20230320112631610"></p><p>跑2000万数据，单台虚拟机的cpu飙升到60%多。</p><h2 id="多台"><a href="#多台" class="headerlink" title="多台"></a>多台</h2><p>事实上，得出的结论与单个虚拟机的结论相似。接下来，我将重点介绍与单个虚拟机报告的不同之处。因为我们使用一个集群（两台机器）来运行快速排序算法，所以使用了这两台机器的所有CPU，从而增强了计算能力，而且我们还可以发现，<strong>当使用2cpu&#x2F;每台vm时，集群状态所需的计算时间更少</strong>，这也意味着我们的集群可以充分利用每个vm的资源来分配操作，对于一些需要复杂操作并且可以划分为子操作的任务来说，这无疑是个好消息。</p><p><img src="image-20230320113005514.png" alt="image-20230320113005514"></p><p>node1 grafana:</p><p><img src="image-20230320112806222.png" alt="image-20230320112806222"></p><p>node2 grafana:</p><p><img src="image-20230320112811804.png" alt="image-20230320112811804"></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
